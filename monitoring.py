#!/usr/bin/env python3
"""
ç›‘æ§è„šæœ¬ - ç›‘æ§Leverçˆ¬è™«æ‰§è¡ŒçŠ¶æ€å’Œæ•°æ®è´¨é‡
"""

import boto3
import json
import time
from datetime import datetime, timedelta
import logging
from typing import Dict, List, Optional

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class LeverScraperMonitor:
    """Leverçˆ¬è™«ç›‘æ§å™¨"""
    
    def __init__(self):
        self.cloudwatch = boto3.client('cloudwatch')
        self.lambda_client = boto3.client('lambda')
        self.s3_client = boto3.client('s3')
        self.dynamodb_client = boto3.client('dynamodb')
        
    def check_lambda_execution(self, function_name: str = 'lever-job-scraper') -> Dict:
        """æ£€æŸ¥Lambdaå‡½æ•°æ‰§è¡ŒçŠ¶æ€"""
        try:
            # è·å–æœ€è¿‘çš„æ‰§è¡Œæ—¥å¿—
            logs_client = boto3.client('logs')
            
            # è·å–æœ€è¿‘çš„æ—¥å¿—æµ
            response = logs_client.describe_log_streams(
                logGroupName=f'/aws/lambda/{function_name}',
                orderBy='LastEventTime',
                descending=True,
                maxItems=5
            )
            
            execution_status = {
                'function_name': function_name,
                'last_execution': None,
                'success_count': 0,
                'error_count': 0,
                'execution_time': 0,
                'jobs_scraped': 0
            }
            
            for stream in response.get('logStreams', []):
                # è·å–æ—¥å¿—äº‹ä»¶
                events_response = logs_client.get_log_events(
                    logGroupName=f'/aws/lambda/{function_name}',
                    logStreamName=stream['logStreamName'],
                    startTime=int((datetime.now() - timedelta(hours=24)).timestamp() * 1000),
                    endTime=int(datetime.now().timestamp() * 1000)
                )
                
                for event in events_response.get('events', []):
                    message = event['message']
                    
                    if 'å¼€å§‹æ‰§è¡ŒLeveræ‹›è˜æ•°æ®çˆ¬è™«' in message:
                        execution_status['last_execution'] = datetime.fromtimestamp(event['timestamp'] / 1000)
                    
                    elif 'çˆ¬è™«æ‰§è¡ŒæˆåŠŸ' in message:
                        execution_status['success_count'] += 1
                        # æå–èŒä½æ•°é‡
                        if 'total_jobs' in message:
                            try:
                                import re
                                match = re.search(r'"total_jobs":\s*(\d+)', message)
                                if match:
                                    execution_status['jobs_scraped'] += int(match.group(1))
                            except:
                                pass
                    
                    elif 'Lambdaå‡½æ•°æ‰§è¡Œå¤±è´¥' in message:
                        execution_status['error_count'] += 1
                    
                    elif 'æ‰§è¡Œæ—¶é—´' in message:
                        try:
                            import re
                            match = re.search(r'æ‰§è¡Œæ—¶é—´:\s*(\d+\.?\d*)', message)
                            if match:
                                execution_status['execution_time'] = float(match.group(1))
                        except:
                            pass
            
            return execution_status
            
        except Exception as e:
            logger.error(f"æ£€æŸ¥Lambdaæ‰§è¡ŒçŠ¶æ€å¤±è´¥: {str(e)}")
            return {'error': str(e)}
    
    def check_s3_data(self, bucket_name: str = 'lever-jobs-data') -> Dict:
        """æ£€æŸ¥S3æ•°æ®å­˜å‚¨çŠ¶æ€"""
        try:
            # åˆ—å‡ºæœ€è¿‘çš„æ–‡ä»¶
            response = self.s3_client.list_objects_v2(
                Bucket=bucket_name,
                MaxKeys=10
            )
            
            s3_status = {
                'bucket_name': bucket_name,
                'total_files': 0,
                'latest_file': None,
                'total_size_mb': 0,
                'files_last_24h': 0
            }
            
            if 'Contents' in response:
                files = response['Contents']
                s3_status['total_files'] = len(files)
                
                # æŒ‰ä¿®æ”¹æ—¶é—´æ’åº
                files.sort(key=lambda x: x['LastModified'], reverse=True)
                
                if files:
                    latest_file = files[0]
                    s3_status['latest_file'] = {
                        'key': latest_file['Key'],
                        'size_mb': latest_file['Size'] / (1024 * 1024),
                        'last_modified': latest_file['LastModified'].isoformat()
                    }
                    
                    # è®¡ç®—æ€»å¤§å°å’Œæœ€è¿‘24å°æ—¶çš„æ–‡ä»¶æ•°
                    cutoff_time = datetime.now(latest_file['LastModified'].tzinfo) - timedelta(hours=24)
                    
                    for file in files:
                        s3_status['total_size_mb'] += file['Size'] / (1024 * 1024)
                        if file['LastModified'] > cutoff_time:
                            s3_status['files_last_24h'] += 1
            
            return s3_status
            
        except Exception as e:
            logger.error(f"æ£€æŸ¥S3æ•°æ®çŠ¶æ€å¤±è´¥: {str(e)}")
            return {'error': str(e)}
    
    def check_dynamodb_data(self, table_name: str = 'lever-jobs') -> Dict:
        """æ£€æŸ¥DynamoDBæ•°æ®çŠ¶æ€"""
        try:
            # è·å–è¡¨ä¿¡æ¯
            table_info = self.dynamodb_client.describe_table(TableName=table_name)
            
            # æ‰«ææœ€è¿‘çš„æ•°æ®
            response = self.dynamodb_client.scan(
                TableName=table_name,
                Select='COUNT'
            )
            
            dynamodb_status = {
                'table_name': table_name,
                'total_items': response.get('Count', 0),
                'scanned_count': response.get('ScannedCount', 0),
                'table_status': table_info['Table']['TableStatus'],
                'items_last_24h': 0
            }
            
            # æ£€æŸ¥æœ€è¿‘24å°æ—¶çš„æ•°æ®
            cutoff_time = datetime.now() - timedelta(hours=24)
            
            # æ‰«ææ‰€æœ‰æ•°æ®ï¼ˆæ³¨æ„ï¼šè¿™å¯èƒ½éœ€è¦åˆ†é¡µå¤„ç†ï¼‰
            scan_response = self.dynamodb_client.scan(TableName=table_name)
            
            for item in scan_response.get('Items', []):
                if 'scraped_at' in item:
                    scraped_time_str = item['scraped_at']['S']
                    try:
                        scraped_time = datetime.fromisoformat(scraped_time_str.replace('Z', '+00:00'))
                        if scraped_time > cutoff_time:
                            dynamodb_status['items_last_24h'] += 1
                    except:
                        pass
            
            return dynamodb_status
            
        except Exception as e:
            logger.error(f"æ£€æŸ¥DynamoDBæ•°æ®çŠ¶æ€å¤±è´¥: {str(e)}")
            return {'error': str(e)}
    
    def check_data_quality(self, table_name: str = 'lever-jobs') -> Dict:
        """æ£€æŸ¥æ•°æ®è´¨é‡"""
        try:
            # æ‰«ææ•°æ®è¿›è¡Œåˆ†æ
            response = self.dynamodb_client.scan(TableName=table_name, Limit=100)
            
            quality_metrics = {
                'total_samples': len(response.get('Items', [])),
                'complete_records': 0,
                'australian_jobs': 0,
                'avg_description_length': 0,
                'companies_with_jobs': set()
            }
            
            total_description_length = 0
            
            for item in response.get('Items', []):
                # æ£€æŸ¥è®°å½•å®Œæ•´æ€§
                required_fields = ['job_title', 'company_name', 'description']
                if all(field in item for field in required_fields):
                    quality_metrics['complete_records'] += 1
                
                # æ£€æŸ¥æ¾³æ´²èŒä½
                if 'company_name' in item:
                    company_name = item['company_name']['S'].lower()
                    australian_keywords = ['australia', 'australian', 'sydney', 'melbourne', 'brisbane']
                    if any(keyword in company_name for keyword in australian_keywords):
                        quality_metrics['australian_jobs'] += 1
                
                # ç»Ÿè®¡æè¿°é•¿åº¦
                if 'description' in item:
                    desc_length = len(item['description']['S'])
                    total_description_length += desc_length
                
                # ç»Ÿè®¡å…¬å¸æ•°é‡
                if 'company_name' in item:
                    quality_metrics['companies_with_jobs'].add(item['company_name']['S'])
            
            if quality_metrics['total_samples'] > 0:
                quality_metrics['avg_description_length'] = total_description_length / quality_metrics['total_samples']
                quality_metrics['companies_with_jobs'] = len(quality_metrics['companies_with_jobs'])
            
            return quality_metrics
            
        except Exception as e:
            logger.error(f"æ£€æŸ¥æ•°æ®è´¨é‡å¤±è´¥: {str(e)}")
            return {'error': str(e)}
    
    def generate_monitoring_report(self) -> str:
        """ç”Ÿæˆç›‘æ§æŠ¥å‘Š"""
        report = f"""# Leverçˆ¬è™«ç›‘æ§æŠ¥å‘Š

ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## ğŸ” Lambdaå‡½æ•°çŠ¶æ€

"""
        
        # Lambdaæ‰§è¡ŒçŠ¶æ€
        lambda_status = self.check_lambda_execution()
        if 'error' not in lambda_status:
            report += f"- å‡½æ•°åç§°: {lambda_status['function_name']}\n"
            report += f"- æœ€åæ‰§è¡Œ: {lambda_status['last_execution']}\n"
            report += f"- æˆåŠŸæ¬¡æ•°: {lambda_status['success_count']}\n"
            report += f"- é”™è¯¯æ¬¡æ•°: {lambda_status['error_count']}\n"
            report += f"- çˆ¬å–èŒä½æ•°: {lambda_status['jobs_scraped']}\n"
        else:
            report += f"- LambdaçŠ¶æ€æ£€æŸ¥å¤±è´¥: {lambda_status['error']}\n"
        
        report += "\n## ğŸ“¦ S3å­˜å‚¨çŠ¶æ€\n\n"
        
        # S3çŠ¶æ€
        s3_status = self.check_s3_data()
        if 'error' not in s3_status:
            report += f"- å­˜å‚¨æ¡¶: {s3_status['bucket_name']}\n"
            report += f"- æ€»æ–‡ä»¶æ•°: {s3_status['total_files']}\n"
            report += f"- æ€»å¤§å°: {s3_status['total_size_mb']:.2f} MB\n"
            report += f"- 24å°æ—¶å†…æ–‡ä»¶æ•°: {s3_status['files_last_24h']}\n"
            if s3_status['latest_file']:
                report += f"- æœ€æ–°æ–‡ä»¶: {s3_status['latest_file']['key']}\n"
                report += f"- æ–‡ä»¶å¤§å°: {s3_status['latest_file']['size_mb']:.2f} MB\n"
        else:
            report += f"- S3çŠ¶æ€æ£€æŸ¥å¤±è´¥: {s3_status['error']}\n"
        
        report += "\n## ğŸ—„ï¸ DynamoDBçŠ¶æ€\n\n"
        
        # DynamoDBçŠ¶æ€
        db_status = self.check_dynamodb_data()
        if 'error' not in db_status:
            report += f"- è¡¨å: {db_status['table_name']}\n"
            report += f"- æ€»è®°å½•æ•°: {db_status['total_items']}\n"
            report += f"- è¡¨çŠ¶æ€: {db_status['table_status']}\n"
            report += f"- 24å°æ—¶å†…è®°å½•æ•°: {db_status['items_last_24h']}\n"
        else:
            report += f"- DynamoDBçŠ¶æ€æ£€æŸ¥å¤±è´¥: {db_status['error']}\n"
        
        report += "\n## ğŸ“Š æ•°æ®è´¨é‡æŒ‡æ ‡\n\n"
        
        # æ•°æ®è´¨é‡
        quality_metrics = self.check_data_quality()
        if 'error' not in quality_metrics:
            report += f"- æ ·æœ¬æ•°é‡: {quality_metrics['total_samples']}\n"
            report += f"- å®Œæ•´è®°å½•: {quality_metrics['complete_records']}\n"
            report += f"- æ¾³æ´²èŒä½: {quality_metrics['australian_jobs']}\n"
            report += f"- å¹³å‡æè¿°é•¿åº¦: {quality_metrics['avg_description_length']:.0f} å­—ç¬¦\n"
            report += f"- æ¶‰åŠå…¬å¸æ•°: {quality_metrics['companies_with_jobs']}\n"
        else:
            report += f"- æ•°æ®è´¨é‡æ£€æŸ¥å¤±è´¥: {quality_metrics['error']}\n"
        
        return report
    
    def send_alert(self, message: str, alert_type: str = 'info'):
        """å‘é€å‘Šè­¦ï¼ˆç¤ºä¾‹å®ç°ï¼‰"""
        # è¿™é‡Œå¯ä»¥é›†æˆSNSã€Slackã€é‚®ä»¶ç­‰å‘Šè­¦æ–¹å¼
        logger.info(f"[{alert_type.upper()}] {message}")
        
        # ç¤ºä¾‹ï¼šå‘é€åˆ°SNS
        try:
            sns_client = boto3.client('sns')
            topic_arn = 'arn:aws:sns:region:account:lever-scraper-alerts'  # éœ€è¦é…ç½®
            
            sns_client.publish(
                TopicArn=topic_arn,
                Message=message,
                Subject=f'Leverçˆ¬è™«å‘Šè­¦ - {alert_type.upper()}'
            )
        except Exception as e:
            logger.error(f"å‘é€å‘Šè­¦å¤±è´¥: {str(e)}")

def main():
    """ä¸»å‡½æ•°"""
    monitor = LeverScraperMonitor()
    
    print("ğŸ” å¼€å§‹ç›‘æ§æ£€æŸ¥...")
    
    # ç”Ÿæˆç›‘æ§æŠ¥å‘Š
    report = monitor.generate_monitoring_report()
    
    # ä¿å­˜æŠ¥å‘Š
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    filename = f'monitoring_report_{timestamp}.md'
    
    with open(filename, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"âœ… ç›‘æ§æŠ¥å‘Šå·²ä¿å­˜åˆ°: {filename}")
    print("\n" + "="*50)
    print(report)
    print("="*50)

if __name__ == "__main__":
    main() 